---
title: "HW3"
author: "Chase Sinclair"
date: "3/20/2020"
output:
  pdf_document: default
  html_document: default
---
## Honor Code
“I have neither given nor received unauthorized assistance on this assignment.” 
-Chase Sinclair

```{r setup, include=FALSE}

knitr::opts_chunk$set(eval = TRUE)
getwd()
setwd("C:/Users/chase/Documents/3654")
```

## Problem 1
(a) Create a subset of the data with female babies named “Mary” from 1880-2014.
(b) Create a subset of the data with female babies named “Sophia” from 1880-2014.
(c) Construct a plot of the proportion of female babies named “Mary” from 1880-2014. On the same plot,
add/overlay a plot of the proportion of female babies named “Sophia” from 1880-2014.
(d) Briefly describe your interpretation of the plot in (c).

## A
```{r 1}
library(babynames)
data("babynames")

library(dplyr)    # alternatively, this also loads %>%

s1 <- babynames %>%
  filter(year >= 1880 & year <= 2014, name =="Mary", sex == "F")
s1
```

## B
```{r 2}
s2 <- babynames %>%
  filter(year >= 1880 & year <= 2014, name =="Sophia", sex == "F")
s2
```

## C
```{r 3}
library(ggplot2)

x = (1:100)/10 
sin= sin(x) 
cos=cos(x) 
data = data.frame(x,sin,cos) 
ggplot(data=s1,aes(x=year,y=prop,color="red"))+geom_line()+ 
  geom_line(data=s2,aes(x=year,y=prop,color="blue")) +
scale_colour_manual(name = "Name", 
                      #Then which colors is going to asume each one. Self explanatory:
                      values =c("red"="red","blue"="blue"), 
                      #And the labels for each, in the same order they were plotted:
                      labels = c("Sophia","Mary"))
```

## D
As you can see from the plot, the female name of "Mary" was very popular in 1880 but declined in popularity afterward. Ever since 1920 there has been a steady decline in babies being called "Mary". On the contrary, "Sophia" was constantly unpopular until the 2000's where it has increased in popularity until present day. 

## Problem 2 (Webscraping)


Extract the data table from the safe routes website at
http://apps.saferoutesinfo.org/legislation_funding%20/state_apportionment.cfm and analyze the data to answer the following questions:

(a) Identify the top 5 states that received the most funds in 2010.
(b) Construct a plot of the data set with years in the x-axis, and total funding received by all states in the y-axis.

## A
```{r 4}
library(XML)
address = "http://apps.saferoutesinfo.org/legislation_funding%20/state_apportionment.cfm"
webhtml<-htmlParse(address)
raw.table<- readHTMLTable(webhtml,stringsAsFactors = FALSE)
data_state <- sapply(raw.table[[1]][,-1], FUN= function(x) #Removed the commas from dataset and removed the first column
  as.character(gsub(",", "", as.character(x), fixed = TRUE) ))

data_state <-as.data.frame(substring(data_state,2), stringsAsFactors=FALSE) #Removing $ logo
data_state$State<-c(raw.table[[1]][,1]) # adding the state name
data_state<-data_state[,c(10,1:9)] # re arrange columns moving state name column back to first column position
names(data_state)[2:10]<-c("Actual2005","Actual2006","Actual2007","Actual2008",
                           "Actual2009","Actual2010","Actual2011", "Actual2012", "total")
data_state = as.data.frame(data_state)
str(data_state)
data_state = data_state[1:51,] # remove the column totals row at the bottom of dataset
for (i in 2:10){
  data_state[,i] = as.numeric(data_state[,i])}
str(data_state)

# Now we can identify the top 5 states that received the most funds in 2010
data_state$State[order(-data_state$Actual2010)[1:5]]


```

## B
```{r 5}

library(ggplot2)
#There are two ways we can do this

v <- data.frame(years = 2005:2012, funding = as.numeric(unlist(data_state[nrow(data_state),])[3:length(unlist(data_state[nrow(data_state),]))-1]))

ggplot(data = v, aes(x = as.factor(years), y = funding))+
  geom_bar(stat = "identity")+
  xlab("Years")

#The second way is much clearer using a barplot which is much clearer than points in this case

ggplot()+
  geom_bar(
    #Create x as a factor:
    aes(x = as.factor(2005:2012), 
        #Y is the columns of all the columns with numerical data
        y = colSums(data_state[,c(3:ncol(data_state)-1)])), 
    #Stat identity since we are specifying what to count 
    stat = "identity")+
  #Appropriate labs:
  xlab("Year")+ylab("Funding Total")

```

## Problem 3 (Statistical learning intuition)
(a) Plot of Petal Length (x-axis) vs Petal Width (y-axis). Briefly describe the relation between petal length
and petal width as you observe from the plot.

(b) Plot of Petal Length (x-axis) vs Petal Width (y-axis), with different colors for the different classes of plants.

(c) Plot of Sepal Length (x-axis) vs Sepal Width (y-axis), with different colors for the different classes of plants.

(d) Observing the plots in (b) and (c), if you had to distinguish between classes by using either petal
dimensions or sepal dimensions, which one would you choose --- petals or sepals, and why?
```{r 6}
library(datasets)
data("iris")
summary(iris)
```

## A
```{r 7}
plot(iris$Petal.Width ~ iris$Petal.Length, pch=20,
     main="Petal Length Vs Petal Width")
```

From the plot we can see that there is a linear relationship between Petal Length and Petal Width. As Petal length increases, petal width increases and vice-versa which makes a lot of sense growing size wise. 

## B
```{r 8}
plot(iris$Petal.Length, iris$Petal.Width, pch=21, 
     bg=c("red","green3","blue")[unclass(iris$Species)], 
     main="Petal Length Vs Petal Width")
legend("topleft",
c("setosa","versicolor", "virginica"),
fill=c("red","green3","blue")
)
```

## C
```{r 9}
plot(iris$Sepal.Length, iris$Sepal.Width, pch=21, 
     bg=c("red","green3","blue")[unclass(iris$Species)], 
     main="Sepal Length vs Sepal Width")
legend("topleft",
c("setosa","versicolor", "virginica"),
fill=c("red","green3","blue")
)
```

## D
If we had to distinguish between classes by using either petal dimensions or sepal dimensions, we would choose petals because it is muhc easier to distinguish between classes due to the similiarity of length and width within each petal class. Each class has their own cluster of data points. On the other hand, sepal classes are more alike between classes and there are not clear clusters for each class.

## Problem 4 (Linear regression with synthetic data)
```{r 10}
setwd("C:/Users/chase/Documents/3654")
linear=read.csv('synthetic_data.csv',header=TRUE,sep = ",")
getwd()
lm1 = lm(y ~ x1 + x2 + x3 + z1 + z2, data=linear)

coef(lm1)
summary(lm1) # the estimation table
```

```{r i}
i = lm(y ~ x1, data=linear)
summary(i) #Summary

plot(x1,y,pch=16) #Plot 
plot(y, pch=16)

data <- cor(mtcars)
abs(data[1,])
```

# 1
The R^2 value we get is 0.4013 which explains the variation in Y.
# 2
The coefficient is 0.673 which is not a very good match to the real coefficient of 0.3
# 3
Linear relationship between x1 and y1, y1 has good dependance on x1

```{r ii}
ii = lm(y ~ x2, data=linear)
summary(ii)
plot(x2,y,pch=16)
plot(y, pch=16)
```

# 1
The R^2 value we get is 0.5215 which explains the variation in Y.
# 2
The coefficient is 0.555 which is not a very good match to the real coefficient of 0.4
# 3
Linear relationship between x2 and y2, y2 has good dependance on x2

```{r iii}
iii = lm(y ~ x3, data=linear)
summary(iii)
plot(x3,y,pch=16)
plot(y, pch=16)
```

# 1
The R^2 value we get is 0.0006 which explains the variation in Y, a very low R^2 value
# 2
The coefficient is -0.026 which is not a very good match to the real coefficient at all.
# 3
There is no linear relationship between x3 and y3, y3 has no dependance on x3

## B
```{r 11}
iv = lm(y ~ x1 + x2, data=linear)
summary(iv)
d <- cor(linear)
abs(d[1,])
```

#  How does model (iv) compare against models (i) and (ii) in terms of:  (1) explaining the variation of y, (2) matching the true coefficients, and (3) determining the dependence of y on x1 and x2

(1):  The R^2 value is 0.5516 which is higher than both of the models (i) and (ii). 
(2): The coefficients in (iv) match to the true coefficients better than the coefficients in (i) and (ii). 0.26 and 0.42 and much closer to 0.3 and 0.4
(3): Y had dependence on x1 and x2

# Explain why you get different slope coefficients for x1 and x2 under model (iv) compared to models (i) and (ii). 
You get different coefficients because you are adding a new predictor to the model thus increasing the efficiency of predicting y which will change the predictor coefficients. 

## C
```{r 12}
v = lm(y ~ x1 + x2 + x3, data=linear)
summary(v)
```

#  How does model (v) compare against models (i), (ii), and (iii) in terms of:  (1) explaining the variation of y, (2) matching the true coefficients, and (3) determining the dependence of y on x1, x2, and x3. 

(1): The R^2 value is just slightly higher however the R^2-Adjusted vale is slightly smaller. 
(2): x1,x2.x3 coefficients are more accurate than in models (i),(ii),(iii), however x3 is still off.
(3): Y does not depend on x3. Non-linear relationship.
## D
```{r 13}
library("MASS")
fit0 = lm(y~1,data=linear)
foo = stepAIC(fit0, scope = list(lower=fit0, upper=v),
              direction = "both")

fit.best = lm(formula(foo), data = linear)
summary(fit.best)
```

# The best model was y ~ x1 + x2 (iv) accoring to stepAIC and overall efficiency. It eliminated the valueless predictor x3 and keo x1 and x2 together being the two best predictors together to predict y.


## Problem 5

(e) Fit linear regression of (i) y1 on x1 (ii) y2 on x2 (iii) y3 on x3 and (iv) y4 on x4. Write down the four fitted regression lines.

(f) Construct the following plots (i) y1 vs x1 (ii) y2 vs x2 (iii) y3 vs x3 and (iv) y4 vs x4.

(g) Use your judgement and describe the discrepancy between the plots and the regression lines.

(h) For each of the four cases in the anscombe dataset, explain whether a linear regression model is appropriate.
```{r 14}
library(datasets)
data("anscombe")
```

## E
```{r 15}
i = lm(y1 ~ x1, data=anscombe)
summary(i)

ii = lm(y2 ~ x2, data=anscombe)
summary(ii)

iii = lm(y3 ~ x3, data=anscombe)
summary(iii)

iv = lm(y4 ~ x4, data=anscombe)
summary(iv)
```

(i): y = 3.0001 + 0.5001x1
(ii): y = 3.001 + 0.500x2
(iii): y = 3.0025 + 0.4997x3
(iv): y = 3.0017 +0.4999x4

## F
```{r 16}
data("anscombe")
library(ggplot2)

plot(y1 ~ x1, data = anscombe, main = "y1 vs x1", pch = 20,  col = "red")
plot(y2 ~ x2, data = anscombe, main = "y2 vs x2", pch = 20,  col = "blue")
plot(y3 ~ x3, data = anscombe, main = "y3 vs x3", pch = 20, col = "purple")
plot(y4 ~ x4, data = anscombe, main = "y4 vs x4", pch = 20, col = "black")

# Adds in a shared x- and a shared y-axis label
mtext(text = "X axis", side = 1, line = 0, outer = TRUE)
mtext(text = "Y axis", side = 2, line = 0, outer = TRUE)
```

## G

# Summary Statistics 
The usual summary statistics for the 4 regression models were virtually identical 

# Plots
a) (y1 vs x1): Plot has a linear relationship and is a good model.
b) (y2 vs x2): The relationship between X and Y is curvilinear, not linear! Not a good model.
c) (y3 vs x3): Plot has perfect linear relationship except for one outlier. Okay model.
d) (y4 vs x4): Plot has no linear relationship and is not a good model. Not a good model.

Scatter-plots revealed that only two of the 4 data sets give us a good/okay model.



## For further review we shall check the residual plots and leverage plots

## H 
# (Scatterplots and summary statistics that are up above are included in this part)
```{r 17}
par(mfrow = c(2, 2))
plot(i)
```

# i) 
Summary statistics are normal. Scatterplot shows a linear relationship. Residual plot is random and has adequate leverage plot which makes the regression model appropriate for the data.

```{r 18}
par(mfrow = c(2, 2))
plot(ii)
```

# ii) 
Sumarry statistics are normal. Scatterplot has a non-linear relationship. The residual plot is u-shaped thus not random. The linear Regression model is not appropriate.

```{r 19}
par(mfrow = c(2, 2))
plot(iii)
```

# iii) 
Summary statistics are normal. Scatterplot is linear except for one outier. Residual plot is not random. Outlier has high leverage. This model is not appropriate for the data.

```{r 20}
par(mfrow = c(2, 2))
plot(iv)
```

# iv) 
Summary statistics are normal. Scatterplot has no linear relationship at all. Residual plot is not random and forms a pattern. All other plots show how this model is not appropriate for our data. 

## Problem 6
(a) Compute the correlation coefficient between mpg and all other features in the dataset. What are the two features most strongly correlated with mpg?

(b) Fit two simple linear regression models: model 1 using the strongest feature from (a) and model 2 using the second strongest feature from (a). Report the linear regression formula (i.e., report the line equation) and the value of R from the two models. If you had to choose between these two models, which one would you choose and why?

(c)Fit a multiple linear regression model with all features. Which features are significant in this model? What is the value of R in this model?

(d) Using stepAIC, identify the best subset of features. Fit a multiple linear regression model using the best subset of features. Write down the regression formula and R for this model. Are any of the features from (a) included in this model? Do they have the same coefficients as they had in model 1 or model 2 from (b)? If the coefficient values have changed, explain why.

## A
```{r 21}
data("mtcars")

data <- cor(mtcars)
abs(data[1,])
```

We can see from this correlation matrix, the two features most strongly correlated with mgp are wt and cyl.


## B
```{r 23}
m1 = lm(mpg ~ wt, data=mtcars)
summary(m1)
```
y = 37.2851 -5.3445x5   (where x5 is wt)
R^2 = 0.7528
```{r 24}
m2 = lm(mpg ~ cyl, data=mtcars)
summary(m2)
```
y = 378846 -2.8758x1   (where x1 is cyl)
R^2 = 0.7262

I would choose the first model because it has a higher R^2 value.

## C
```{r 25}
all = lm(mpg ~ cyl + disp + hp + drat + wt + qsec + vs + am + gear + carb, data = mtcars)
summary(all)
```
There are no features that are significant in the full model. The R^2 is 0.869 with an adjusted R^2 of 80.66%.

## D
```{r 26}
library("MASS")
model0 = lm(mpg~1,data=mtcars)
foo = stepAIC(model0, scope = list(lower=model0, upper=all),
              direction = "both")

fit.best = lm(formula(foo), data = mtcars)
summary(fit.best)
```

# Write down the regression formula and R^2 for this model

y = 38.75179 -3.16x5 -0.941x1 -0.018x3     (where x5 is wt, x1 is cyl, x3 is hp)
The R^2 for this model is 0.8431.

# Are any of the features from (a) included in this model? 
Yes, the top two features most strongly correlated with mgp are included in this regression model.

# Do they have the same coefficients as they had in model 1 or model 2 from (b)?

Yes,

From model 1, wt had a coefficient of -5.3445 where as now it has a coefficient of -3.166.
From model 2, cyl had a coefficient of -2.8758 where as now it has a coefficient of -0.941.

The coefficient vaues have changed because new predictor values were formulated into the model through reduction. 






